/************************************************************
*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*
*************************************************************/

package singa;

// To start a training job, all we need is a JobProto object.
// It should contain following fields
//  - Job Name (name)
//      the name to identify the job
//  - NeuralNet (neuralnet)
//      the neural network structure contains a set of layers
//  - Train One Batch (alg)
//      the training algorithm
//  - Updater (updater)
//      the protocol for updating parameters at server side
//  - Cluster Topology (cluster)
//      the distributed topology of workers/servers
//  - Training Steps (train_steps)
//      the number of training iteration
//  All other fields/functions are optional, e.g., test, checkpoint
//
message JobProto {
  // job name, e.g., "cifar10-dcnn", "mnist-mlp"
  //required string name = 1;
  optional string name = 1;
  // neural net consits of a set of connected layers
  //required NetProto neuralnet = 3;
  optional NetProto neuralnet = 3;
  // algorithm for computing gradients over one mini-batch
  //required AlgProto train_one_batch = 5;
  optional AlgProto train_one_batch = 5;
  // configuration of SGD updater, including learning rate, etc.
  //required UpdaterProto updater = 7;
  optional UpdaterProto updater = 7;
  // cluster toplogy conf
  required ClusterProto cluster = 9;
  // total num of steps for training
  //required int32 train_steps = 16;
  optional int32 train_steps = 16;
  // frequency of displaying training info
  optional int32 disp_freq = 17 [default = 0];
  // GPU device IDs for use, if fewer than workers per procs, some workers run
  // on GPU and the rest run on CPU.
  repeated int32 gpu = 18;

  // frequency of test, e.g., do test every 100 training steps
  optional int32 test_freq = 20 [default = 0];
  // total num of steps for testing all test data;
  // TODO(wangwei): set -1 for test forever
  optional int32 test_steps =  21 [default = 0];
  // frequency of validation, e.g., do validation every 100 training steps
  optional int32 validate_freq = 25 [default = 0];
  // total num of steps for validating all validation data
  optional int32 validate_steps = 26 [default = 0];
  // frequency of checkpoint
  optional int32 checkpoint_freq = 30 [default = 0];

  // for loading checkpoint files to init parameters
  repeated string checkpoint_path = 60;
  // send parameters to servers after training for this num of steps
  optional int32 warmup_steps = 61 [default = 0];
  // display debug info
  optional bool debug = 62 [default = false];
  // reset the version of params loaded from checkpoint file to step
  optional bool reset_param_version = 63 [default = true];
  // set num of threads used by openblas
  optional int32 num_openblas_threads = 64 [default = 1];

  // start checkpoint after this num steps
  optional int32 checkpoint_after = 80 [default = 0];
  // start display after this num steps
  optional int32 disp_after =  81[default = 0];
  // start test after this num steps
  optional int32 test_after = 82 [default = 0];
  // start validation after this num steps
  optional int32 validate_after = 83 [default = 0];

  // for internal use
  // users typically do not touch following fields

  // resume flag
  optional bool resume = 90 [default = false];
  // last snapshot step
  optional int32 step = 91 [default = 0];
  // job id allocated by zookeeper
  optional int32 id = 92 [default = -1];

  extensions 101 to 200;
}

// Protos used by JobProto
// -----------------------

message AlgProto {
  // algorithms calculating gradients for one mini-batch/iteration
  optional AlgType alg = 1 [default = kUserAlg];
  // user defined algorithm
  optional string user_alg = 2;
  // for setting CD fields
  optional CDProto cd_conf = 10;

  extensions 101 to 200;
}
message NetProto {
  repeated LayerProto layer = 1;
  // partitioning type for parallelism
  optional int32 partition_dim = 20 [default = 0];
}

message UpdaterProto {
  // built-in updater type
  optional UpdaterType type = 1 [default = kUserUpdater];
  // user-defned updater type
  optional string user_type = 2;

  // configuration for RMSProp algorithm
  optional RMSPropProto rmsprop_conf = 3;

  // learning rate generator
  optional LRGenProto learning_rate = 11;
  optional float momentum = 31 [default = 0];
  optional float weight_decay = 32 [default = 0];

  // used to avoid divide by 0, i.e. x/(y+delta)
  optional float delta = 35 [default = 0.00000001];

  extensions 101 to 200;
}

message ClusterProto {
  optional int32 nworker_groups = 1 [default = 1];
  optional int32 nserver_groups = 2 [default = 1];
  optional int32 nworkers_per_group = 3 [default = 1];
  optional int32 nservers_per_group = 4 [default = 1];
  optional int32 nworkers_per_procs = 5 [default = 1];
  optional int32 nservers_per_procs = 6 [default = 1];
  // local workspace for checkpoint files and vis files
  //required string workspace = 10;
  optional string workspace = 10;

  // servers and workers in different processes?
  optional bool server_worker_separate = 20 [default = false];

  // sync frequency between server groups
  optional int32 sync_freq = 21 [default = 1];

  // port number used by ZeroMQ
  optional int32 start_port = 60 [default = 6723];
  // share memory space between worker groups in one procs
  optional bool share_memory = 62 [default = true];

  // poll time in milliseconds
  optional int32 poll_time = 81 [default = 100];
}

message CDProto {
  //number of steps for gibbs sampling
  optional int32 cd_k = 1 [default = 1];
}

message LayerProto {
  // the layer name used for identification
  required string name = 1;
  // source layer names
  repeated string srclayers = 3;
  // parameters, e.g., weight matrix or bias vector
  repeated ParamProto param = 12;
  // all layers are included in the net structure for training phase by default.
  // some layers like data layer for loading test data are not used by training
  // phase should be removed by setting the exclude field.
  repeated Phase exclude = 15;
  // exclude field is deprecated, please use include field instead!!!
  // some layers like data layer for loading test data are not used by training
  // in this case, only test phase should be included by setting the include field.
  repeated Phase include = 14;
  // type of built-in layer
  optional LayerType type = 20 [default = kUserLayer];
  // type of user layer
  optional string user_type =21;

  // proto for the specific layer
  // configuration for input layers
  optional ActivationProto activation_conf = 54;
  // configuration for argsort layer
  optional ArgSortProto argsort_conf = 52;
  // configuration for concate layer
  optional ConcateProto concate_conf = 46;
  // configuration for convolution layer
  optional ConvolutionProto convolution_conf = 30;
  // configuration for dummy layer
  optional DummyProto dummy_conf = 55;
  // configuration for dropout layer
  optional DropoutProto dropout_conf = 33;
  // configuration for inner product layer
  optional InnerProductProto innerproduct_conf = 34;
  // configuration for local response normalization layer
  optional DataProto lmdbdata_conf = 35;
  // configuration for local response normalization layer
  optional LRNProto lrn_conf = 45;
  // configuration for mnist parser layer
  optional MnistProto mnist_conf = 36;
  // configuration for pooling layer
  optional PoolingProto pooling_conf = 37;
  // configuration for prefetch layer
  optional PrefetchProto prefetch_conf = 44;
  // configuration for rbmhid layer
  optional RBMProto rbm_conf = 49;
  // configuration for rectified linear unit layer
  optional ReLUProto relu_conf = 38;
  // configuration for rgb image parser layer
  optional RGBImageProto rgbimage_conf = 39;
  // configuration for data layer
  optional DataProto sharddata_conf = 32;
  // configuration for slice layer
  optional SliceProto slice_conf = 41;
  // configuration for softmax layer
  optional SoftmaxProto softmax_conf = 53;
  // configuration for softmax loss layer
  optional SoftmaxLossProto softmaxloss_conf = 40;
  // configuration for split layer
  optional SplitProto split_conf = 42;
  // configuration for store input layers
  optional StoreProto store_conf = 51;


  // overrides the partition dimension for neural net
  optional int32 partition_dim = 60 [default = -1];
  // names of parameters shared from other layers
  optional int32 partition_id = 90 [default = 0];
  // num of partitions for this layer
  optional int32 num_partitions = 91 [default = 1];

  extensions 101 to 200;
}

// weight matrix should be defined before bias vector
// TODO(wangwei): separate conf for diff init method
message ParamProto {
  // used for identifying the same params from diff models and display deug info
  optional string name =  1 [default = ""];
  // for built-in Param
  optional ParamType type = 3 [default = kParam];
  // for user-defined Param
  optional string user_type = 4;

  optional ParamGenProto init =5;
    // multiplied on the global learning rate.
  optional float lr_scale = 15 [default = 1];
  // multiplied on the global weight decay.
  optional float wd_scale = 16 [default = 1];

  // name of the owner param from which this param shares the values
  optional string share_from = 60;

  // used interally
  optional int32 id = 90;
  // used internally
  optional int32 owner = 91 [default = -1];
  // partition dimension, -1 for no partition
  optional int32 partition_dim = 92;
  // usually, the program will infer the param shape
  repeated int32 shape = 93;

  extensions 101 to 200;
}

// ---------------------------
// protos for different layers
// ---------------------------
// learning rate generator proto
message LRGenProto {
  // user-defined change method
  optional ChangeMethod type = 1 [default = kUserChange];
  optional string user_type = 2;

  optional float base_lr = 3 [default = 0.01];

  optional FixedStepProto fixedstep_conf = 40;
  optional StepProto step_conf = 41;
  optional LinearProto linear_conf = 42;
  optional ExponentialProto exponential_conf = 43;
  optional InverseProto inverse_conf = 44;
  optional InverseTProto inverset_conf = 45;

  extensions 101 to 200;
}

message ParamGenProto {
  optional InitMethod type = 1 [default = kUserInit];
  optional string user_type =2;
  // constant init
  optional float value = 3 [default = 1];
  // for gaussian sampling
  optional float mean = 4 [default = 0];
  optional float std = 5 [default = 1];
  // for uniform sampling
  optional float low = 8 [default = -1];
  optional float high = 9 [default = 1];

  extensions 101 to 200;
}

enum ActivationType {
  RELU = 1;
  SIGMOID = 2;
  TANH = 3;
  STANH = 4;
}

message ActivationProto {
  optional ActivationType type = 1 [default = RELU];
}

message RGBImageProto {
  // scale factor for each pixel
  optional float scale = 1 [default = 1.0];
  // size after cropping
  optional int32 cropsize = 2 [default = 0];
  // mirror the image
  optional bool mirror = 3 [default = false];
  // meanfile path
  optional string meanfile = 4 [default = ""];
}

message PrefetchProto {
  repeated LayerProto sublayers = 1;
}

message SplitProto {
  optional int32 num_splits = 1 [default = 1];
}

message StoreProto {
  optional string backend = 1;
  optional string path = 2;
  optional string separator = 3 [default = ","];
  optional string mean_file = 4;
  optional string std_file = 5;
  optional float mean_value = 6;
  optional float std_value = 7;
  optional int32 batchsize = 8 [default = 1];
  repeated int32 shape = 9;
  optional bool encoded = 10 [default = false];
  optional int32 random_skip = 11 [default = 0];
  optional bool has_label = 12 [default = true];
}

message SoftmaxLossProto {
  // computing accuracy against topk results
  optional int32 topk = 1 [default = 1];
  // loss scale factor
  optional float scale = 30 [default = 1];
}

message ArgSortProto {
  // keep labels with topk scores
  optional int32 topk = 1 [default = 1];
}

message ConcateProto {
  optional int32 concate_dim = 1 [default = 0];
  optional int32 num_concates = 2 [default = 1];
} 

message ConvolutionProto {
  // The number of outputs for the layer
  optional int32 num_filters = 1;
  // the kernel height/width
  optional int32 kernel = 2 [default = 3];
  // The padding height/width
  optional int32 pad = 30 [default = 0];
  // the stride
  optional int32 stride = 31 [default = 1];

  optional int32 kernel_x = 41 [default = 3];
  optional int32 kernel_y = 42 [default = 3];

  optional int32 pad_x = 44 [default = 0];
  optional int32 pad_y = 45 [default = 0];

  optional int32 stride_x = 47 [default = 1];
  optional int32 stride_y = 48 [default = 1];

  // cudnn workspace size in MB
  optional int32 workspace_byte_limit = 50 [default = 512];
}

message DataProto {
  // path to the data file/folder, absolute or relative to the workspace
  required string path = 2;
  // batch size.
  required int32 batchsize = 4;
  // skip [0,random_skip] records
  optional int32 random_skip = 30 [default = 0];
}

message MnistProto {
  // normalization x/norm_a
  required float norm_a = 1 [default = 1];
  // normalization x-norm_b
  required float norm_b = 2 [default = 0];

  // elastic distortion
  optional int32 kernel = 30 [default = 0];
  optional float sigma = 31 [default = 0];
  optional float alpha = 32 [default = 0];
  // rotation or horizontal shearing
  optional float beta = 33 [default = 0];
  // scaling
  optional float gamma = 34 [default = 0];
  // scale to this size as input for deformation
  optional int32 resize = 35 [default = 0] ;
  optional int32 elastic_freq = 36 [default = 0];
}

message DummyProto {
  // shape of data and grad blobs
  optional bool input = 1 [default = false];
  optional bool output = 2 [default = false];
  repeated int32 shape = 3;
}

// Message that stores parameters used by DropoutLayer
message DropoutProto {
  // dropout ratio
  optional float dropout_ratio = 30 [default = 0.5];
}

message RBMProto {
  required int32 hdim = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional bool gaussian = 3 [default = false]; // use gaussian sampling or not
}

// Message that stores parameters used by InnerProductLayer
message InnerProductProto {
  // number of outputs for the layer
  required int32 num_output = 1;
  // use bias vector or not
  optional bool bias_term = 30 [default = true];
  // transpose or not
  optional bool transpose = 31 [default = false];
}

message LRNProto {
  // local response size
  required int32 local_size = 1 [default = 5];
  // scale factor
  optional float alpha = 31 [default = 1.0];
  // exponential number
  optional float beta = 32 [default = 0.75];
  // offset
  optional float knorm = 34 [default = 1.0];
}

message PoolingProto {
  // The kernel size (square)
  optional int32 kernel= 1 [default = 3];
  enum PoolMethod {
    MAX = 0;
    AVG = 1;
  }
  // The pooling method
  optional PoolMethod pool = 30 [default = MAX];
  // The padding size
  optional uint32 pad = 31 [default = 0];
  // The stride
  optional uint32 stride = 32 [default = 2];

  optional int32 kernel_x = 41 [default = 3];
  optional int32 kernel_y = 42 [default = 3];

  optional int32 pad_x = 44 [default = 0];
  optional int32 pad_y = 45 [default = 0];

  optional int32 stride_x = 47 [default = 2];
  optional int32 stride_y = 48 [default = 2];
}


message ReLUProto {
  // Ref. Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013).
  // Rectifier nonlinearities improve neural network acoustic models.
  // In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.
  optional float negative_slope = 1 [default = 0];
}

message SliceProto {
  optional int32 slice_dim = 1 [default = 0];
  optional int32 num_slices = 2 [default = 1];
}

message SoftmaxProto {
  // Can be used to do softmax over each channel of one image by setting it to
  // be the size of the second dimension (the first dimension is batchsize).
  optional int32 num_softmax_per_instance = 1 [default = 1];
}

message RMSPropProto {
 // history=history*rho_+(1-rho_)*(grad*grad_scale);
  required float rho = 1;
}

message FixedStepProto {
  repeated int32 step = 28;
  // lr = step_lr[i] if current step >= step[i]
  repeated float step_lr = 29;
}

message StepProto {
  // lr = base_lr * gamma^(step/change_freq)
  required float gamma = 35 [default = 1];
  // lr = base_lr * gamma^(step/change_freq)
  required int32 change_freq = 40;
}

message LinearProto {
  // lr = (1 - step / freq) * base_lr + (step / freq) * final_lr
  required int32 change_freq= 40;
  // lr = (1 - step / freq) * base_lr + (step / freq) * final_lr
  required float final_lr = 39;
}

message ExponentialProto {
  // lr = base / 2^(step/change_freq)
  required int32 change_freq = 40;
}

message InverseTProto {
  // lr = base_lr / (1+step/final_lr)
  required float final_lr = 39;
}
message InverseProto {
  // lr = base_lr*(1+gamma*step)^(-pow)
  required float gamma = 1 [default = 1];
  // lr = base_lr*(1+gamma*step)^(-pow)
  required float pow = 2 [default = 0];
}
message UniformProto {
  optional float low = 1 [default = -1];
  optional float high = 2 [default = 1];
}
message GaussianProto {
  optional float mean = 1 [default = 0];
  optional float std = 2 [default = 1];
}


// --------------
// All Enum Types
// --------------

enum ChangeMethod {
  kFixed = 0;
  kInverseT = 1;
  kInverse = 2;
  kExponential = 3;
  kLinear = 4;
  kStep = 5;
  kFixedStep = 6;
  // For user defiend change method
  kUserChange = 100;
}

enum InitMethod {
  // fix the values of all parameters  a constant in the value field
  kConstant = 0;
  // sample gaussian with std and mean
  kGaussian = 1;
  // uniform sampling between low and high
  kUniform = 2;
  // from Toronto Convnet, let a=1/sqrt(fan_in), w*=a after generating from
  // Gaussian distribution
  kGaussianSqrtFanIn = 4;
  // from Toronto Convnet, rectified linear activation, let
  // a=sqrt(3)/sqrt(fan_in), range is [-a, +a]; no need to set value=sqrt(3),
  // the program will multiply it.
  kUniformSqrtFanIn = 5;
  // from Theano MLP tutorial, let a=sqrt(6/(fan_in+fan_out)). for tanh
  // activation, range is [-a, +a], for sigmoid activation, range is
  // [-4a, +4a], put the scale factor to value field.
  // <a href="http://deeplearning.net/tutorial/mlp.html"> Theano MLP</a>
  kUniformSqrtFanInOut = 6;

  // For user defined init method
  kUserInit = 101;
}

enum LayerType {
  // Input/Output layers
  //  - Load records from file, database
  kRecordInput = 29;
  kCSVInput = 30;
  kCSVOutput = 32;
  kRecordOutput = 33;
  kImagePreprocess = 31;
  kPrefetch = 19;

  // deprecated input layers
  kLMDBData = 17;
  kShardData = 3;
  kLabel = 18;
  kMnist = 7;
  kRGBImage = 10;
  // Neuron layers
  //  - Feature transformation
  kAccuracy = 36;
  kArgSort = 35;
  kConvolution = 1;
  kCConvolution = 27;
  kCPooling = 28;
  kCudnnConv = 50;
  kCudnnPool = 51;
  kCudnnLRN = 52;
  kCudnnSoftmax = 53;
  kCudnnActivation = 54;
  kCudnnSoftmaxLoss = 55;
  kDropout = 4;
  kDummy = 20;
  kInnerProduct = 5;
  kLRN = 6;
  kPooling = 8;
  kReLU = 9;
  kRBMVis = 23;
  kRBMHid = 24;
  kSigmoid = 26;
  kSTanh = 14;
  kSoftmax = 34;
  // Loss layers
  //  - Compute objective loss
  kSoftmaxLoss = 11;
  kEuclideanLoss = 25;
  // Connection layers
  //  - Connect layers when neural net is partitioned
  kBridgeDst = 16;
  kBridgeSrc = 15;
  kConcate = 2;
  kSlice = 12;
  kSplit = 13;


  // Indicate the user defined layer. Users should configure user_type
  kUserLayer = 102;
}

enum PartitionType {
  kDataPartition = 0;
  kLayerPartition = 1;
  kNone = 2;
}

enum Phase {
  kUnknown = 0;
  kTrain = 1;
  kVal = 2;
  kTest= 4;
  // postivie phase for contrastive divergence algorithm
  kPositive = 8;
  // negative phase for contrastive divergence algorithm
  kNegative = 16;
  kForward = 32;
  kBackward = 64;
  kLoss = 128;
  kDeploy = 256;
}

enum ParamType {
  // built-in Param
  kParam = 0;
  // user-defined Param
  kUser = 103;
}

enum AlgType {
  // Back-propagation algorithm for feed-forward models, e.g., CNN and RNN
  kBP = 1;
  // Contrastive Divergence algorithm for RBM, DBM, etc.
  kCD = 2;
  // For user defined algorithm.
  kUserAlg = 104;
}

enum UpdaterType {
  // noraml SGD with momentum and weight decay
  kSGD = 1;
  // adaptive subgradient, http://www.magicbroom.info/Papers/DuchiHaSi10.pdf
  kAdaGrad = 2;
  // http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  kRMSProp = 3;
  // Nesterov first optimal gradient method
  kNesterov = 4;
  // For user defined updater
  kUserUpdater = 105;
}
